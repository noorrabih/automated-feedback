{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e74c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e9114",
   "metadata": {},
   "source": [
    "# Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "03e90945",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZAEBUC_COR = pd.read_csv('ZAEBUC-v1.0/AR-all.extracted.corrected.analyzed.corrected-FINAL.tsv', encoding='utf_8',sep='\\t')\n",
    "ZAEBUC_aligned = pd.read_csv('ZAEBUC-v1.0/AR-all.alignment-FINAL.tsv', encoding='utf_8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5305924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## group raw essays\n",
    "ZAEBUC_RAW = ZAEBUC_aligned[['Document', 'Raw']].dropna( subset = ['Raw']).groupby('Document').agg({'Raw': lambda x: ' '.join(np.array(x, dtype=str))})\n",
    "ZAEBUC_RAW = ZAEBUC_RAW.rename_axis('Document').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "docs = ZAEBUC_COR['Document'].apply(lambda x: x if x.startswith('<') else np.nan).dropna()\n",
    "\n",
    "grades = []\n",
    "word_count = []\n",
    "\n",
    "for xml in docs:\n",
    "    if xml != \"</doc>\":\n",
    "        doc = xmltodict.parse(xml)\n",
    "        grades.append(doc[\"doc\"][\"@CEFR\"])\n",
    "        word_count.append(doc[\"doc\"][\"@word_count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a4fd8",
   "metadata": {},
   "source": [
    "# Working with Raw Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cd7d1",
   "metadata": {},
   "source": [
    "## Using doc2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "401b56c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ف+', 'تنفست', 'ال+', 'صعداء']\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "# The tokenizer expects pre-tokenized text\n",
    "sentence = 'فتنفست الصعداء'.split()\n",
    "\n",
    "# Load a pretrained disambiguator to use with a tokenizer\n",
    "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "\n",
    "# By specifying `split=True`, the morphological tokens are output as seperate\n",
    "# strings.\n",
    "tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "e4f0de04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DDE', 'ف+', 'تنفست']"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('DDE فتنفست '.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ae3d2f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize raw data and add it to the dataframe\n",
    "ZAEBUC_RAW['Tokenized'] = ZAEBUC_RAW['Raw'].apply(lambda x: tokenizer.tokenize(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get doc2vec vectors for raw data\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#tag the documents\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(ZAEBUC_RAW['Tokenized'])]\n",
    "\n",
    "#train the model\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    \n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    \n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c0491693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/_f2ygxrx2lb72hdyk0tkc_lr0000gn/T/ipykernel_48153/37932420.py:6: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  vectors.append(model.docvecs[i])\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "# get vectors for the raw data\n",
    "vectors = []\n",
    "for i in range(len(ZAEBUC_RAW)):\n",
    "    vectors.append(model.docvecs[i])\n",
    "\n",
    "#add vectors to dataframe\n",
    "ZAEBUC_RAW['Doc2Vec Embeddings'] = vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f02e7d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/_f2ygxrx2lb72hdyk0tkc_lr0000gn/T/ipykernel_48153/3494704710.py:22: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  raw1 = ZAEBUC_RAW['Raw'][index1]\n",
      "/var/folders/lw/_f2ygxrx2lb72hdyk0tkc_lr0000gn/T/ipykernel_48153/3494704710.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  raw2 = ZAEBUC_RAW['Raw'][index2]\n"
     ]
    }
   ],
   "source": [
    "# get the pair with highest cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "# get all combinations of vectors\n",
    "comb = combinations(vectors, 2)\n",
    "# get the pair with highest cosine similarity\n",
    "max_cos = 1\n",
    "for i in list(comb):\n",
    "    cos = cosine_similarity([i[0]], [i[1]])\n",
    "    if cos > max_cos:\n",
    "        max_cos = cos\n",
    "        pair = i\n",
    "\n",
    "for k in range(len(vectors)):\n",
    "    if(all(vectors[k] == pair[0])):\n",
    "        index1 = k\n",
    "    if(all(vectors[k] == pair[1])):\n",
    "        index2 = k\n",
    "\n",
    "# get the raw data of the pair\n",
    "raw1 = ZAEBUC_RAW['Raw'][index1]\n",
    "raw2 = ZAEBUC_RAW['Raw'][index2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6efcb",
   "metadata": {},
   "source": [
    "## Using Bert Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5068b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the raw data into sentences with delimiter . or ،\n",
    "\n",
    "ZAEBUC_RAW['Sentences'] = ZAEBUC_RAW['Raw'].apply(lambda x: x.replace('،', '.').split('.'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "aefa0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "docs = ZAEBUC_COR['Document'].apply(lambda x: x if x.startswith('<') else np.nan).dropna()\n",
    "\n",
    "grades = []\n",
    "word_count = []\n",
    "\n",
    "for xml in docs:\n",
    "    if xml != \"</doc>\":\n",
    "        doc = xmltodict.parse(xml)\n",
    "        grades.append(doc[\"doc\"][\"@CEFR\"])\n",
    "        word_count.append(doc[\"doc\"][\"@word_count\"])\n",
    "ZAEBUC_RAW['grade'] = grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "926fd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay2chunks(essay: str):\n",
    "    sents = []\n",
    "    split_essay = essay.replace('.', '.+').split('+')\n",
    "    if('' in split_essay):\n",
    "        split_essay.remove('')\n",
    "    for sent in split_essay:\n",
    "        if(len(sent.split())<50):\n",
    "            sents.append(sent)\n",
    "        else:\n",
    "            for i in range(len(sent.split())//50):\n",
    "                sents.append(' '.join(sent.split()[i*50:(i+1)*50]))\n",
    "            sents.append(' '.join(sent.split()[(i+1)*50:]))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "eadfb2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZAEBUC_RAW['3 Sentences'] = ZAEBUC_RAW['Sentences'].apply(lambda x: [x[i:i+3]  for i in range(0, len(x), 3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c645ac35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Raw</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>grade</th>\n",
       "      <th>3 Sentences</th>\n",
       "      <th>chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR-030-268469</td>\n",
       "      <td>وسائل التواصل الاجتماعي لها اضرار و فوائد كثير...</td>\n",
       "      <td>[وسائل التواصل الاجتماعي لها اضرار و فوائد كثي...</td>\n",
       "      <td>B1</td>\n",
       "      <td>[[وسائل التواصل الاجتماعي لها اضرار و فوائد كث...</td>\n",
       "      <td>[وسائل التواصل الاجتماعي لها اضرار و فوائد كثي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR-030-386369</td>\n",
       "      <td>تعد وسائل التواصل الاجتماعي من اكبر المؤثرات ع...</td>\n",
       "      <td>[تعد وسائل التواصل الاجتماعي من اكبر المؤثرات ...</td>\n",
       "      <td>B2</td>\n",
       "      <td>[[تعد وسائل التواصل الاجتماعي من اكبر المؤثرات...</td>\n",
       "      <td>[تعد وسائل التواصل الاجتماعي من اكبر المؤثرات ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR-030-81027</td>\n",
       "      <td>قام انتشارالوساءل للتواصل الاجتماعية بشكل كبير...</td>\n",
       "      <td>[قام انتشارالوساءل للتواصل الاجتماعية بشكل كبي...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[[قام انتشارالوساءل للتواصل الاجتماعية بشكل كب...</td>\n",
       "      <td>[قام انتشارالوساءل للتواصل الاجتماعية بشكل كبي...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR-030-81757</td>\n",
       "      <td>وسائل التواصل الاجتماعي لقد تطورت وسائل المعرف...</td>\n",
       "      <td>[وسائل التواصل الاجتماعي لقد تطورت وسائل المعر...</td>\n",
       "      <td>B2</td>\n",
       "      <td>[[وسائل التواصل الاجتماعي لقد تطورت وسائل المع...</td>\n",
       "      <td>[وسائل التواصل الاجتماعي لقد تطورت وسائل المعر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR-030-83625</td>\n",
       "      <td>من اشهر وساءل الاتصال بالآخرين هي الاجتماعية،</td>\n",
       "      <td>[من اشهر وساءل الاتصال بالآخرين هي الاجتماعية, ]</td>\n",
       "      <td>Unassessable</td>\n",
       "      <td>[[من اشهر وساءل الاتصال بالآخرين هي الاجتماعية...</td>\n",
       "      <td>[من اشهر وساءل الاتصال بالآخرين هي الاجتماعية،]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>AR-130-99351</td>\n",
       "      <td>ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ال...</td>\n",
       "      <td>[ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ا...</td>\n",
       "      <td>B2</td>\n",
       "      <td>[[ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ...</td>\n",
       "      <td>[ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>AR-130-99438</td>\n",
       "      <td>وسائل التواصل الاجتماعي منذ انتشار وسائل التوا...</td>\n",
       "      <td>[وسائل التواصل الاجتماعي منذ انتشار وسائل التو...</td>\n",
       "      <td>B2</td>\n",
       "      <td>[[وسائل التواصل الاجتماعي منذ انتشار وسائل الت...</td>\n",
       "      <td>[وسائل التواصل الاجتماعي منذ انتشار وسائل التو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>AR-130-99442</td>\n",
       "      <td>وسائل التواصل الإجتماعي .إنّ التواصل الإجتماعي...</td>\n",
       "      <td>[وسائل التواصل الإجتماعي , إنّ التواصل الإجتما...</td>\n",
       "      <td>B2</td>\n",
       "      <td>[[وسائل التواصل الإجتماعي , إنّ التواصل الإجتم...</td>\n",
       "      <td>[وسائل التواصل الإجتماعي ., إنّ التواصل الإجتم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>AR-130-99590</td>\n",
       "      <td>التسامح أمر مهم جداً يجب على الفرد اخذه بجدية،...</td>\n",
       "      <td>[التسامح أمر مهم جداً يجب على الفرد اخذه بجدية...</td>\n",
       "      <td>B1</td>\n",
       "      <td>[[التسامح أمر مهم جداً يجب على الفرد اخذه بجدي...</td>\n",
       "      <td>[التسامح أمر مهم جداً يجب على الفرد اخذه بجدية...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>AR-130-99787</td>\n",
       "      <td>التواصل الإجتماعي عبارة عن مجموعة من التكنولوج...</td>\n",
       "      <td>[التواصل الإجتماعي عبارة عن مجموعة من التكنولو...</td>\n",
       "      <td>B1</td>\n",
       "      <td>[[التواصل الإجتماعي عبارة عن مجموعة من التكنول...</td>\n",
       "      <td>[التواصل الإجتماعي عبارة عن مجموعة من التكنولو...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Document                                                Raw  \\\n",
       "0    AR-030-268469  وسائل التواصل الاجتماعي لها اضرار و فوائد كثير...   \n",
       "1    AR-030-386369  تعد وسائل التواصل الاجتماعي من اكبر المؤثرات ع...   \n",
       "2     AR-030-81027  قام انتشارالوساءل للتواصل الاجتماعية بشكل كبير...   \n",
       "3     AR-030-81757  وسائل التواصل الاجتماعي لقد تطورت وسائل المعرف...   \n",
       "4     AR-030-83625      من اشهر وساءل الاتصال بالآخرين هي الاجتماعية،   \n",
       "..             ...                                                ...   \n",
       "209   AR-130-99351  ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ال...   \n",
       "210   AR-130-99438  وسائل التواصل الاجتماعي منذ انتشار وسائل التوا...   \n",
       "211   AR-130-99442  وسائل التواصل الإجتماعي .إنّ التواصل الإجتماعي...   \n",
       "212   AR-130-99590  التسامح أمر مهم جداً يجب على الفرد اخذه بجدية،...   \n",
       "213   AR-130-99787  التواصل الإجتماعي عبارة عن مجموعة من التكنولوج...   \n",
       "\n",
       "                                             Sentences         grade  \\\n",
       "0    [وسائل التواصل الاجتماعي لها اضرار و فوائد كثي...            B1   \n",
       "1    [تعد وسائل التواصل الاجتماعي من اكبر المؤثرات ...            B2   \n",
       "2    [قام انتشارالوساءل للتواصل الاجتماعية بشكل كبي...            A2   \n",
       "3    [وسائل التواصل الاجتماعي لقد تطورت وسائل المعر...            B2   \n",
       "4     [من اشهر وساءل الاتصال بالآخرين هي الاجتماعية, ]  Unassessable   \n",
       "..                                                 ...           ...   \n",
       "209  [ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ا...            B2   \n",
       "210  [وسائل التواصل الاجتماعي منذ انتشار وسائل التو...            B2   \n",
       "211  [وسائل التواصل الإجتماعي , إنّ التواصل الإجتما...            B2   \n",
       "212  [التسامح أمر مهم جداً يجب على الفرد اخذه بجدية...            B1   \n",
       "213  [التواصل الإجتماعي عبارة عن مجموعة من التكنولو...            B1   \n",
       "\n",
       "                                           3 Sentences  \\\n",
       "0    [[وسائل التواصل الاجتماعي لها اضرار و فوائد كث...   \n",
       "1    [[تعد وسائل التواصل الاجتماعي من اكبر المؤثرات...   \n",
       "2    [[قام انتشارالوساءل للتواصل الاجتماعية بشكل كب...   \n",
       "3    [[وسائل التواصل الاجتماعي لقد تطورت وسائل المع...   \n",
       "4    [[من اشهر وساءل الاتصال بالآخرين هي الاجتماعية...   \n",
       "..                                                 ...   \n",
       "209  [[ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ...   \n",
       "210  [[وسائل التواصل الاجتماعي منذ انتشار وسائل الت...   \n",
       "211  [[وسائل التواصل الإجتماعي , إنّ التواصل الإجتم...   \n",
       "212  [[التسامح أمر مهم جداً يجب على الفرد اخذه بجدي...   \n",
       "213  [[التواصل الإجتماعي عبارة عن مجموعة من التكنول...   \n",
       "\n",
       "                                                chunks  \n",
       "0    [وسائل التواصل الاجتماعي لها اضرار و فوائد كثي...  \n",
       "1    [تعد وسائل التواصل الاجتماعي من اكبر المؤثرات ...  \n",
       "2    [قام انتشارالوساءل للتواصل الاجتماعية بشكل كبي...  \n",
       "3    [وسائل التواصل الاجتماعي لقد تطورت وسائل المعر...  \n",
       "4      [من اشهر وساءل الاتصال بالآخرين هي الاجتماعية،]  \n",
       "..                                                 ...  \n",
       "209  [ظهور الأجهزة الإلكترونية أدى إلى ظهور وسائل ا...  \n",
       "210  [وسائل التواصل الاجتماعي منذ انتشار وسائل التو...  \n",
       "211  [وسائل التواصل الإجتماعي ., إنّ التواصل الإجتم...  \n",
       "212  [التسامح أمر مهم جداً يجب على الفرد اخذه بجدية...  \n",
       "213  [التواصل الإجتماعي عبارة عن مجموعة من التكنولو...  \n",
       "\n",
       "[214 rows x 6 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ZAEBUC_RAW['chunks'] = ZAEBUC_RAW['Raw'].apply(lambda x: essay2chunks(x))\n",
    "ZAEBUC_RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "477fa6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Document', '3 Sentences', 'grade'])\n",
    "for i in range(len(ZAEBUC_RAW)):\n",
    "    df = pd.concat((df,pd.DataFrame({'Document': [ZAEBUC_RAW['Document'][i]]*len(ZAEBUC_RAW['3 Sentences'][i]), '3 Sentences': ZAEBUC_RAW['3 Sentences'][i],\n",
    "                                     'grade': [ZAEBUC_RAW['grade'][i]]*len(ZAEBUC_RAW['3 Sentences'][i])})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "994a69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['3 Sentences'] = df['3 Sentences'].apply(lambda x: ' '.join(x))\n",
    "# drop empty sentences\n",
    "df = df[df['3 Sentences'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5925ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dataframe for chunked data and grades\n",
    "chunks_df = pd.DataFrame(columns=['Document', 'chunks', 'grade'])\n",
    "for i in range(len(ZAEBUC_RAW)):\n",
    "    chunks_df = pd.concat((chunks_df,pd.DataFrame({'Document': [ZAEBUC_RAW['Document'][i]]*len(ZAEBUC_RAW['chunks'][i]), 'chunks': ZAEBUC_RAW['chunks'][i],\n",
    "                                     'grade': [ZAEBUC_RAW['grade'][i]]*len(ZAEBUC_RAW['chunks'][i])})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "10dd9c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Get the Arabert embeddings for the chunked data\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
    "model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
    "\n",
    "def get_embeddings(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    return last_hidden_states.detach().numpy()\n",
    "\n",
    "chunks_df['Arabert Embeddings'] = chunks_df['chunks'].apply(lambda x: get_embeddings(x))\n",
    "chunks_df['Arabert Embeddings'] = chunks_df['Arabert Embeddings'].apply(lambda x: x[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605931e5",
   "metadata": {},
   "source": [
    "# Working with Human Annotated Data for a Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "07f68aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_essays= ZAEBUC_COR[['Document', 'Word']].dropna( subset = ['Word']).groupby('Document').agg({'Word': lambda x: ' '.join(np.array(x, dtype=str))})\n",
    "corrected_essays = corrected_essays.rename_axis('Document').reset_index()\n",
    "corrected_essays.rename(columns={'Word': 'Corrected essay'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acaf3b2",
   "metadata": {},
   "source": [
    "## Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6553cf78",
   "metadata": {},
   "source": [
    "### Using AraBert Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "3af58a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split corrected data into chunks and get the Arabert embeddings\n",
    "df = corrected_essays.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "e2ca916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentences'] = corrected_essays['Corrected essay'].apply(lambda x: essay2chunks(x))\n",
    "df['grade'] = ZAEBUC_RAW['grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "ee699fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = pd.DataFrame(columns=['Document', 'sent', 'grade'])\n",
    "for i in range(len(df)):\n",
    "    sent_df = pd.concat((sent_df,pd.DataFrame({'Document': [df['Document'][i]]*len(df['Sentences'][i]), 'sent': df['Sentences'][i],\n",
    "                                     'grade': [df['grade'][i]]*len(df['Sentences'][i])})))\n",
    "\n",
    "sent_df['Arabert Embeddings'] = sent_df['sent'].apply(lambda x: get_embeddings(x))\n",
    "sent_df['Arabert Embeddings'] = sent_df['Arabert Embeddings'].apply(lambda x: x[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "27c2e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty or nan sentences\n",
    "sent_df = sent_df[sent_df['sent'] != '']\n",
    "sent_df = sent_df[sent_df['sent'].notna()]\n",
    "\n",
    "#drop sentnces with less than 3 words\n",
    "sent_df = sent_df[sent_df['sent_len'] > 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e66017",
   "metadata": {},
   "source": [
    "### Using doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c0653217",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corrected_essays.rename_axis('Document').reset_index()\n",
    "df.rename(columns={'Word': 'Corrected essays'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "238de0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add grades to the dataframe for each document\n",
    "df = pd.merge(df, ZAEBUC_RAW[['Document', 'grade']], on='Document')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b63df3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lw/_f2ygxrx2lb72hdyk0tkc_lr0000gn/T/ipykernel_48153/4212022539.py:35: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  vectors.append(model.docvecs[i])\n"
     ]
    }
   ],
   "source": [
    "# tokenize and get embeddings for the corrected essays\n",
    "df['Tokenized'] = df['Corrected essays'].apply(lambda x: tokenizer.tokenize(x.split()))\n",
    "\n",
    "#tag the documents\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(df['Tokenized'])]\n",
    "\n",
    "#train the model\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    \n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    \n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "# get vectors for the corrected essays\n",
    "vectors = []\n",
    "for i in range(len(df)):\n",
    "    vectors.append(model.docvecs[i])\n",
    "\n",
    "#add vectors to dataframe\n",
    "df['Doc2Vec Embeddings'] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "cc6c4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Document', 'grade', 'Doc2Vec Embeddings']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13053c6",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "672efc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_essays_df = ZAEBUC_COR[['Document','Auto_Tokenization']].dropna(subset='Auto_Tokenization').groupby(by = 'Document').agg({'Auto_Tokenization': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "66158384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scores : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Applying TFIDF\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1, 1) )\n",
    "doc2vec = vectorizer.fit_transform(tokenized_essays_df['Auto_Tokenization'])\n",
    "doc2vec = (doc2vec.toarray())\n",
    "print(\"\\n\\nScores : \\n\", doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "6cbd8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Document': tokenized_essays_df.index, 'TFIDF Embeddings': doc2vec.tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01707c01",
   "metadata": {},
   "source": [
    "### Other Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "50bcbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Auto_POS']= np.array(ZAEBUC_COR[['Document', 'Auto_POS']].dropna(subset= 'Auto_POS').groupby(by = 'Document', as_index = True).agg({'Auto_POS': ' '.join})['Auto_POS'])\n",
    "df['Auto_POS'] = df['Auto_POS'].dropna().apply(lambda x: x.replace('+', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "0114ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_df = ZAEBUC_aligned[['Document','Operation']].groupby('Document').aggregate({'Operation': (lambda x: 1- np.sum(x=='NO_CHANGE')/len(x) )}).rename(columns = {'Operation':'error_rate'})\n",
    "df = pd.merge(df, error_rate_df, on='Document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d643d920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scores : \n",
      " [[0.19436809 0.         0.07920732 ... 0.         0.         0.02296755]\n",
      " [0.29714894 0.0171133  0.08351155 ... 0.         0.         0.        ]\n",
      " [0.32720461 0.         0.08333737 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.1976791  0.04306373 0.07004921 ... 0.         0.0155246  0.        ]\n",
      " [0.2220965  0.         0.0808098  ... 0.         0.02865505 0.        ]\n",
      " [0.26979631 0.02503347 0.07635085 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Applying TFIDF\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1, 2))\n",
    "pos2vec = vectorizer.fit_transform(df['Auto_POS'])\n",
    "pos2vec = (pos2vec.toarray())\n",
    "print(\"\\n\\nScores : \\n\", pos2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "889f9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POS2Vec Embeddings'] = pos2vec.tolist()\n",
    "df = df.drop(columns = ['Auto_POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5c93bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add word count to the dataframe\n",
    "df['word count'] = corrected_essays['Corrected essay'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "28729308",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((np.array(df['Doc2Vec Embeddings'].tolist()), np.array(df['error_rate'].tolist()).reshape(-1,1), np.array(df['POS2Vec Embeddings'].tolist()), np.array(df['word count'].tolist()).reshape(-1,1)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "34e46485",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = df['grade']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0ff54",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e4a0c",
   "metadata": {},
   "source": [
    "## Using entire essays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8100e3e",
   "metadata": {},
   "source": [
    "### Using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "f48135f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0  0  0]\n",
      " [ 0  1  2  0  0]\n",
      " [ 0  0 14  2  0]\n",
      " [ 0  0 11 10  0]\n",
      " [ 0  0  0  1  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A0       1.00      1.00      1.00         1\n",
      "          A2       1.00      0.33      0.50         3\n",
      "          B1       0.52      0.88      0.65        16\n",
      "          B2       0.77      0.48      0.59        21\n",
      "          C1       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.63        43\n",
      "   macro avg       0.86      0.64      0.68        43\n",
      "weighted avg       0.71      0.63      0.62        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier for X and grades\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, grades, test_size = 0.20)\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "14279ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = list(map(lambda x:'A0' if (x == 'Unassessable') else x, grades))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "be39186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_to_num = { 'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6, 'A0': 0}\n",
    "num_grades = list(map(lambda x: grades_to_num[x], grades))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d75184cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, num_grades, test_size = 0.20, stratify = num_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "90bcc633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## given a dataset X and grades y, return a dataset of pair-wise differences and labels (+,-) \n",
    "def to_pairs(X, y):\n",
    "    paired_X = list()\n",
    "    paired_y = list()\n",
    "    for i in range(len(X)):\n",
    "        for k in range(i+1, len(X), 1):\n",
    "                paired_X.append(np.subtract(X[i], X[k]))\n",
    "                paired_y.append(y[i] > y[k])\n",
    "    return paired_X, paired_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "69efe14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_diff, y_train_diff = to_pairs(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e4ae1e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_diff, y_test_diff = to_pairs(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "3b35437e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[504 143]\n",
      " [ 99 157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.78      0.81       647\n",
      "        True       0.52      0.61      0.56       256\n",
      "\n",
      "    accuracy                           0.73       903\n",
      "   macro avg       0.68      0.70      0.69       903\n",
      "weighted avg       0.75      0.73      0.74       903\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'C': [0.1,1, 10, 100], 'kernel' : ['linear']}\n",
    "\n",
    "# grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\n",
    "# grid.fit(X_train_diff,y_train_diff)\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train_diff, y_train_diff)\n",
    "\n",
    "y_pred = svclassifier.predict(X_test_diff)\n",
    "\n",
    "print(confusion_matrix(y_test_diff,y_pred))\n",
    "print(classification_report(y_test_diff,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84cfb5b",
   "metadata": {},
   "source": [
    "### Final Mapping to Grades with a Linear Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "310589cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.5813953488372093\n",
      "[[ 0  0  1  0  0]\n",
      " [ 1  0  1  0  0]\n",
      " [ 2  0 14  6  0]\n",
      " [ 0  0  7  9  0]\n",
      " [ 0  0  0  0  2]]\n"
     ]
    }
   ],
   "source": [
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(svc_fitted_X_train, y_train) \n",
    "svm_predictions = svm_model_linear.predict(svc_fitted_X_test)\n",
    "\n",
    "# model accuracy for X_test   \n",
    "accuracy = svm_model_linear.score(svc_fitted_X_test, y_test) \n",
    "print('accuracy= {}'.format(accuracy))\n",
    "# creating a confusion matrix \n",
    "cm = confusion_matrix(y_test, svm_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e7129cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.61      0.64      0.62        22\n",
      "           4       0.60      0.56      0.58        16\n",
      "           5       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.58        43\n",
      "   macro avg       0.44      0.44      0.44        43\n",
      "weighted avg       0.58      0.58      0.58        43\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862df0d7",
   "metadata": {},
   "source": [
    "## Using Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68125446",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0094e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, grades, test_size = 0.20)\n",
    "\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
