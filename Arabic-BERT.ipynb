{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "At9JJwEoYmR3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHSdOx6IleAi"
   },
   "source": [
    "# Import Zaebuc data and split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCSNrCm3mVxr",
    "outputId": "e272f6d6-bdcb-4628-b641-210a462f5787"
   },
   "outputs": [],
   "source": [
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfMfuKcflnh1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#file_path = path to Zaebuc/AR-all.extracted.corrected.analyzed.corrected-FINAL.tsv\n",
    "\n",
    "all_extracted = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "import xmltodict\n",
    "docs = all_extracted['Document'].apply(lambda x: x if x.startswith('<') else np.nan).dropna()\n",
    "\n",
    "grades = []\n",
    "word_count = []\n",
    "\n",
    "for xml in docs:\n",
    "    if xml != \"</doc>\":\n",
    "        doc = xmltodict.parse(xml)\n",
    "        grades.append(doc[\"doc\"][\"@CEFR\"])\n",
    "        word_count.append(doc[\"doc\"][\"@word_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrS4ECOQbATK"
   },
   "outputs": [],
   "source": [
    "#file_path = path to Zaebuc/AR-all.alignment-FINAL.tsv\n",
    "cor_raw_aligned = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkspFqlfbOsk"
   },
   "outputs": [],
   "source": [
    "raw_essays = cor_raw_aligned.dropna(subset=['Raw']).groupby('Document').agg({'Raw': ' '.join})\n",
    "raw_essays['grade'] = grades\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "raw_train, raw_test = train_test_split(raw_essays, test_size=0.2, random_state=42, stratify = raw_essays['grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFTuaL3zYuZ6"
   },
   "source": [
    "# Import original and augmented essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBDvP7v6Y6vq"
   },
   "outputs": [],
   "source": [
    "# raw_essays = pd.read_csv('raw_essays.csv') load \n",
    "augmented_essays = pd.read_csv('augmented_essays(2).csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_essays = raw_essays[raw_essays['grade']!= 'Unassessable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfO1OcQ-gDUm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "raw_train, raw_test = train_test_split(raw_essays, test_size=0.2, random_state=42, stratify = raw_essays['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6HCBHsX9u_M"
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat((augmented_essays.rename({'to_grade': 'grade'}, axis = 1).drop('from_grade', axis = 1), raw_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0OP0DgknodN"
   },
   "source": [
    "# Train Arabic-BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4RHDDKthzew",
    "outputId": "d665acf4-cd19-4b81-cd1d-de98a8137821"
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.16\n",
    "%pip install arabert\n",
    "%pip install farasapy\n",
    "%pip install pyarabic==0.6.14\n",
    "%pip install sentencepiece==0.1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KqtmpsgiTTG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (AutoConfig, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, BertTokenizer, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers.data.processors.utils import InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9a_e4hNhoX7-"
   },
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, text, target, model_name, max_len, label_map):\n",
    "      super(ClassificationDataset).__init__()\n",
    "      \"\"\"\n",
    "      Args:\n",
    "      text (List[str]): List of the training text\n",
    "      target (List[str]): List of the training labels\n",
    "      tokenizer_name (str): The tokenizer name (same as model_name).\n",
    "      max_len (int): Maximum sentence length\n",
    "      \"\"\"\n",
    "      self.text = text\n",
    "      self.target = target\n",
    "      self.tokenizer_name = model_name\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "      self.max_len = max_len\n",
    "      self.label_map = label_map\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "      return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "      text = str(self.text[item])\n",
    "      text = \" \".join(text.split())\n",
    "\n",
    "      inputs = self.tokenizer(\n",
    "          text,\n",
    "          max_length=self.max_len,\n",
    "          padding='max_length',\n",
    "          truncation=True\n",
    "      )\n",
    "      return InputFeatures(**inputs,label=self.label_map[self.target[item]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_Q9X4L4oiQ4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import and define arabert preprocessor\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "# model_name = 'aubmindlab/bert-base-arabertv2'\n",
    "model_name = 'asafaya/bert-base-arabic'\n",
    "# arabic_prep = ArabertPreprocessor(model_name)\n",
    "tok = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeqCYoC7rBTE"
   },
   "outputs": [],
   "source": [
    "raw_train, raw_val = train_test_split(raw_train, test_size=0.1, random_state=42, stratify = raw_train['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7FV3eGDo1Lj"
   },
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "label_map = {'A1' : 0, 'A2': 0, 'B1':1, 'B2':2, 'C1':3,'C2': 3 }\n",
    "train_dataset = ClassificationDataset(\n",
    "    X_train['Raw'].tolist(),\n",
    "    X_train['grade'].tolist(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )\n",
    "\n",
    "val_dataset = ClassificationDataset(\n",
    "    raw_val['Raw'].tolist(),\n",
    "    raw_val['grade'].tolist(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )\n",
    "test_dataset = ClassificationDataset(\n",
    "    raw_test['Raw'].tolist(),\n",
    "    raw_test['grade'].tolist(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7hn3femsVkM"
   },
   "outputs": [],
   "source": [
    "# path_to_best_model = \n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(path_to_best_model, return_dict=True, num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o2R8KYQsYiT"
   },
   "outputs": [],
   "source": [
    "# inverse_label_map = {0:'Unassessable', 1:'A1', 2:'A2', 3:'B1',4: 'B2',5: 'C1',6:'C2'}\n",
    "inverse_label_map = { 0:'A', 1:'B1', 2:'B2',3: 'C'}\n",
    "\n",
    "def compute_metrics(p): #p should be of type EvalPrediction\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  assert len(preds) == len(p.label_ids)\n",
    "  print(classification_report([inverse_label_map[x] for x in p.label_ids],[inverse_label_map[x] for x in preds]))\n",
    "  print(confusion_matrix(p.label_ids,preds))\n",
    "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\n",
    "  macro_precision = precision_score(p.label_ids,preds,average='macro')\n",
    "  macro_recall = recall_score(p.label_ids,preds,average='macro')\n",
    "  acc = accuracy_score(p.label_ids,preds)\n",
    "  return {\n",
    "      'macro_f1' : macro_f1,\n",
    "      'accuracy': acc,\n",
    "      'macro_precision': macro_precision,\n",
    "      'macro_recall': macro_recall\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8KKIYg5seIG"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.deterministic=True\n",
    "  torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UspNw0uHsiXk"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./14-12\",\n",
    "    adam_epsilon = 1e-8,\n",
    "    learning_rate = 1e-5,\n",
    "    fp16 = True, # enable this when using V100 or T4 GPU\n",
    "    per_device_train_batch_size = 32, # up to 64 on 16GB with max len of 128\n",
    "    per_device_eval_batch_size = 128,\n",
    "    gradient_accumulation_steps = 2, # use this to scale batch size without needing more memory\n",
    "    num_train_epochs= 10,\n",
    "    warmup_ratio = 0,\n",
    "    do_eval = True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True, # this allows to automatically get the best model at the end based on whatever metric we want\n",
    "    metric_for_best_model = 'accuracy',\n",
    "    greater_is_better = True,\n",
    "    seed = 25\n",
    "  )\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "td8S-bH9s_9T"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model_init(),\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "UrLyznkCtJYt",
    "outputId": "18c0ac16-5537-4894-b5bf-1226bf34b61b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCt8BY0m0qlH"
   },
   "source": [
    "# Train AraBERT at Sentence Level and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4SbLJoILzTf"
   },
   "outputs": [],
   "source": [
    "chunks_df = pd.DataFrame()\n",
    "X_train['split'] = [x.split() for x in X_train['Raw'].tolist()]\n",
    "for i in range(len(X_train)):\n",
    "  chunks = []\n",
    "  for j in range(len(X_train['split'][i])// 30):\n",
    "    chunks.append(X_train['split'].tolist()[j*30:(j+1)*30])\n",
    "  chunks.append(X_train['split'].tolist()[(j+1)*30:])\n",
    "  chunks_df = pd.concat([chunks_df, pd.DataFrame({'Document': [X_train['Document'][i]]*len(chunks), 'chunk': chunks, 'grade': [X_train['grade'][i]] * len(chunks)})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-67FgQ1vHg6"
   },
   "outputs": [],
   "source": [
    "chunks_df['chunk'] = chunks_df['chunk'].apply(lambda x: ' '.join(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHqUqHe1vSZI"
   },
   "outputs": [],
   "source": [
    "chunks_train, chunks_val = train_test_split(chunks_df, test_size=0.1, random_state=42, stratify = chunks_df['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSge44jvvoSu"
   },
   "outputs": [],
   "source": [
    "max_len = 60\n",
    "# label_map = {'Unassessable': 0, 'A2': 1, 'B1':2, 'B2':3, 'C1':4}\n",
    "train_dataset = ClassificationDataset(\n",
    "    chunks_train['chunk'].tolist(),\n",
    "    chunks_train['grade'].tolist(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )\n",
    "val_dataset = ClassificationDataset(\n",
    "    chunks_val['chunk'].tolist(),\n",
    "    chunks_val['grade'].tolist(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrIJ3uO_v9i_"
   },
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z2Aof6FGwFX5",
    "outputId": "192d6bb1-ac90-4230-b743-2e09063d12af"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model_init(),\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I7JtngT2wLc2",
    "outputId": "a5e1c56b-ca03-4eab-ecc2-a968fef5a424"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5mmfH4w1rGh"
   },
   "source": [
    "### map back to final grades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "Zs5XcRLRE-Wn",
    "outputId": "79565a69-ff9d-4858-cd57-6f2a0069049a"
   },
   "outputs": [],
   "source": [
    "chunks_val['preds'] = trainer.predict(val_dataset).predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BJtcWOQ8HY2y",
    "outputId": "42b80d48-98f1-4ec2-f4d0-ba06aeb2f51a"
   },
   "outputs": [],
   "source": [
    "chunks_val.groupby('Document').agg({'preds': lambda x: np.max(x), 'grade': lambda x: x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKQNLzWb6Z23"
   },
   "outputs": [],
   "source": [
    "# chunks_val['grade'] =chunks_val['grade'].apply(lambda x: np.array(x).reshape(-1,1)[0])\n",
    "# chunks_val['preds'] =chunks_val['preds'].apply(lambda x: inverse_label_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLmoNcqI7IYT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khpNzB3RzFwP"
   },
   "outputs": [],
   "source": [
    "chunks_df_test = pd.DataFrame()\n",
    "raw_test['split'] =raw_test['Raw'].apply(lambda x: x.split())\n",
    "for i in range(len(raw_test)):\n",
    "  chunks = []\n",
    "  for j in range(len(raw_test['split'][i]) // 30):\n",
    "    chunks.append(raw_test['split'][i][j*30:(j+1)*30])\n",
    "  chunks.append(raw_test['split'][i][(j+1)*30:])\n",
    "  chunks_df_test = pd.concat([chunks_df_test, pd.DataFrame({'Document': [raw_test.index[i]]*len(chunks), 'chunk': chunks, 'grade': [raw_test['grade'][i]] * len(chunks)})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-o3WyIl-dOU"
   },
   "outputs": [],
   "source": [
    "test_dataset = ClassificationDataset(\n",
    "    chunks_df_test['chunk'].tolist(),\n",
    "    chunks_df_test['grade'].tolist(),\n",
    "    model_name,\n",
    "    max_len,\n",
    "    label_map\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "djua0NXYy9gR",
    "outputId": "a9e29d48-5203-40bc-a8b8-159e6fddacde"
   },
   "outputs": [],
   "source": [
    "chunks_df_test['prediction'] = trainer.predict(test_dataset).predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeGOSwm1-xrU"
   },
   "outputs": [],
   "source": [
    "chunks_df_test = chunks_df_test.groupby('Document').agg({'prediction': lambda x: inverse_label_map[np.floor(np.mean(x)+0.5)], 'grade': lambda x: x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITymHKd58Hlh"
   },
   "outputs": [],
   "source": [
    "chunks_df_test['grade'] =chunks_df_test['grade'].apply(lambda x: np.array(x).reshape(-1,1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYAOtah29ZKT"
   },
   "outputs": [],
   "source": [
    "chunks_df_test['grade'] = chunks_df_test['grade'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTOFpMqJ87Qr",
    "outputId": "d0a693a7-cbc2-43e4-a37b-ea527d374aa3"
   },
   "outputs": [],
   "source": [
    "print(classification_report(chunks_df_test['grade'], chunks_df_test['prediction']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "kBmD8ghM0wqk"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
