{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS, Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('ZAEBUC-v1.0/AR-all.alignment-FINAL.tsv', encoding='utf_8',sep='\\t')\n",
    "\n",
    "documents = df['Document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samer_df = pd.read_csv('./samer-readability-lexicon/SAMER-Readability-Lexicon.tsv', encoding='utf_8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_for_documents = {}\n",
    "\n",
    "for document in set(documents):\n",
    "    words_for_documents[document] = df.loc[df['Document'] == document , 'Raw'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "HERE\n",
      "HERE\n"
     ]
    }
   ],
   "source": [
    "# join sentences\n",
    "sentences_per_document = {}\n",
    "sentence = []\n",
    "document_ids = []\n",
    "\n",
    "for document_num in words_for_documents:\n",
    "    document = words_for_documents[document_num]\n",
    "    doc = []\n",
    "    for i in range(len(document)):\n",
    "        if document[i] != document[i]:\n",
    "            sentence.append('n')\n",
    "            if i == len(document) - 1:\n",
    "                doc.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            sentence.append(document[i])\n",
    "            if document[i][-1] == '.' or document[i][-1] == '،':\n",
    "                doc.append(sentence)\n",
    "                sentence = []\n",
    "            elif i == len(document) - 1:\n",
    "                doc.append(sentence)\n",
    "                sentence = []\n",
    "    sentences_per_document[document_num] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(component='MorphologyDB', name='calima-msa-r13', description='Database for analyzing Modern Standard Arabic', license='GPL v2', version='0.4.0', path=PosixPath('/Users/noor/.camel_tools/data/morphology_db/calima-msa-r13'))\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "msa_d3_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, scheme='atbtok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document, tokenize each sentence\n",
    "tokenized_sentences_per_document = {}\n",
    "for document_num in sentences_per_document:\n",
    "    sentences = sentences_per_document[document_num]\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentences.append(msa_d3_tokenizer.tokenize(sentence))\n",
    "    tokenized_sentences_per_document[document_num] = tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the above but with tokenized_sentences_per_document\n",
    "fixed_tokenized_sentences_per_document = {}\n",
    "for document_num in tokenized_sentences_per_document:\n",
    "    sentences = tokenized_sentences_per_document[document_num]\n",
    "    fixed_tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sentence = []\n",
    "        for i in range(len(sentence)):\n",
    "            if '+' in sentence[i]:\n",
    "                a = sentence[i].replace('_', '').split('+')\n",
    "                new_sentence += a[:-1]\n",
    "                if sentence[i][-1] == '.':\n",
    "                    new_sentence.append(a[-1][:-1])\n",
    "                    new_sentence.append('.')\n",
    "                else:\n",
    "                    new_sentence.append(a[-1])\n",
    "            else:\n",
    "                if sentence[i] == '.':\n",
    "                    new_sentence.append('.')\n",
    "                elif sentence[i][-1] == '.':\n",
    "                    new_sentence.append(sentence[i][:-1])\n",
    "                    new_sentence.append('.')\n",
    "                else:\n",
    "                    new_sentence.append(sentence[i])\n",
    "        fixed_tokenized_sentences.append(new_sentence)\n",
    "\n",
    "        \n",
    "    fixed_tokenized_sentences_per_document[document_num] = fixed_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(component='MorphologyDB', name='calima-msa-r13', description='Database for analyzing Modern Standard Arabic', license='GPL v2', version='0.4.0', path=PosixPath('/Users/noor/.camel_tools/data/morphology_db/calima-msa-r13'))\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_sentences = []\n",
    "lex_tagged_sentences = []\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tagger.default import DefaultTagger\n",
    "\n",
    "mled = MLEDisambiguator.pretrained()\n",
    "pos_tagger = DefaultTagger(mled, 'pos')\n",
    "lex_tagger = DefaultTagger(mled, 'lex')\n",
    "\n",
    "# for each document, tag each sentence\n",
    "pos_tagged_sentences_per_document = {}\n",
    "lex_tagged_sentences_per_document = {}\n",
    "\n",
    "for document_num in fixed_tokenized_sentences_per_document:\n",
    "    sentences = fixed_tokenized_sentences_per_document[document_num]\n",
    "    pos_tagged_sentences = []\n",
    "    lex_tagged_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        pos_tagged_sentences.append(pos_tagger.tag(sentences[i]))\n",
    "        lex_tagged_sentences.append(lex_tagger.tag(sentences[i]))\n",
    "    pos_tagged_sentences_per_document[document_num] = pos_tagged_sentences\n",
    "    lex_tagged_sentences_per_document[document_num] = lex_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = []\n",
    "\n",
    "for document_num in pos_tagged_sentences_per_document:\n",
    "    sentences = pos_tagged_sentences_per_document[document_num]\n",
    "    lex_sentences = lex_tagged_sentences_per_document[document_num]\n",
    "    for i in range(len(sentences)):\n",
    "        sentences_df.append({'Document': document_num, 'Sentence': fixed_tokenized_sentences_per_document[document_num][i], 'POS_of_sentence': sentences[i], 'LEX_of_Sentence': lex_sentences[i]})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with the above but each word alone\n",
    "words_df = []\n",
    "for document_num in pos_tagged_sentences_per_document:\n",
    "    sentences = pos_tagged_sentences_per_document[document_num]\n",
    "    lex_sentences = lex_tagged_sentences_per_document[document_num]\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences[i])):\n",
    "            words_df.append({'Document': document_num, 'Sentence': fixed_tokenized_sentences_per_document[document_num][i][j], 'POS': sentences[i][j], 'LEX': lex_sentences[i][j]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame(sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.to_csv('sentences_features.csv', encoding='utf_8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_levels(lemmas_pos):\n",
    "    levels = []\n",
    "    for lemma in lemmas_pos:\n",
    "        level = samer_df.loc[samer_df['lemma#pos'] == lemma , 'readability (rounded average)']\n",
    "        if level.empty:\n",
    "            # add the readability score to the dataframe\n",
    "            levels.append(0)\n",
    "            \n",
    "        else:\n",
    "            levels.append(level.values[0])\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df['lemma#pos'] = words_df['LEX'] + '#' + words_df['POS']\n",
    "words_df['readability'] = get_levels(words_df['lemma#pos'])\n",
    "\n",
    "words_df.to_csv('words_features.csv', encoding='utf_8',sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get ratio of POS per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document, get the ration of 'noun' POS tags\n",
    "def get_ratio_of_pos(pos):\n",
    "\n",
    "    pos_ratio_per_document = {}\n",
    "    for document_num in pos_tagged_sentences_per_document:\n",
    "        sentences = pos_tagged_sentences_per_document[document_num]\n",
    "        pos_count = 0\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                if sentences[i][j] == pos:\n",
    "                    pos_count += 1\n",
    "        pos_ratio_per_document[document_num] = pos_count/len(words_for_documents[document_num])\n",
    "        pos_count = 0\n",
    "    return pos_ratio_per_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open error_rates_per_document.csv\n",
    "error_rates_df = pd.read_csv('error_rates_per_document.csv', encoding='utf_8',sep='\\t')\n",
    "\n",
    "# map get_ratio_of_pos('noun') to the df with the correct document number\n",
    "nouns_ratio = get_ratio_of_pos('noun')\n",
    "verbs_ratio = get_ratio_of_pos('verb')\n",
    "adj_ratio = get_ratio_of_pos('adj')\n",
    "prep_ratio = get_ratio_of_pos('prep')\n",
    "conj_ratio = get_ratio_of_pos('conj')\n",
    "adv_ratio = get_ratio_of_pos('adv')\n",
    "\n",
    "error_rates_df['ratio_of_nouns'] = error_rates_df['Document'].map(nouns_ratio)\n",
    "error_rates_df['ratio_of_verbs'] = error_rates_df['Document'].map(verbs_ratio)\n",
    "error_rates_df['ratio_of_verbs'] = error_rates_df['Document'].map(adj_ratio)\n",
    "error_rates_df['ratio_of_prep'] = error_rates_df['Document'].map(prep_ratio)\n",
    "error_rates_df['ratio_of_conj'] = error_rates_df['Document'].map(conj_ratio)\n",
    "error_rates_df['ratio_of_adv'] = error_rates_df['Document'].map(adv_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rates_df.to_csv('rates_per_document.csv', encoding='utf_8',sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
